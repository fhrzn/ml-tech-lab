{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/affan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/affan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import urllib.request\n",
    "\n",
    "import re\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('alice.txt', <http.client.HTTPMessage at 0x7f9ae36bf7c0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download\n",
    "urllib.request.urlretrieve('http://www.gutenberg.org/files/11/11-0.txt', 'alice.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Content for Each Chapter\n",
    "We want to extract only the contents for each chapter. After some quick look, we found a pattern that each chapter started by `CHAPTER XX` title. And in the end of all chapter there is `THE END`. We can use both of them as switch to start and stop writing chapter's content.\n",
    "<br><br>\n",
    "Note that here we iterate dataset line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start trigger\n",
    "start = 'CHAPTER'\n",
    "# stop trigger\n",
    "stop = 'THE END'\n",
    "\n",
    "# all chapters\n",
    "all_chapters = []\n",
    "# chapter story\n",
    "chapters = []\n",
    "# write switch\n",
    "write, writenew = False, False\n",
    "\n",
    "for line in open('./alice.txt', 'r', encoding='utf-8').readlines():\n",
    "\n",
    "    # found 'THE END'\n",
    "    if stop in line:\n",
    "        # add the last chapter\n",
    "        all_chapters.append(' '.join(chapters))\n",
    "        break\n",
    "    \n",
    "    # found new 'CHAPTER'\n",
    "    if start in line:\n",
    "        \n",
    "        # trigger to write\n",
    "        write = True\n",
    "        writenew = True\n",
    "\n",
    "        # collect existing chapter\n",
    "        if len(chapters) > 0:\n",
    "            all_chapters.append(' '.join(chapters))            \n",
    "        \n",
    "        # reset chapters\n",
    "        chapters = []\n",
    "\n",
    "    # chapter's content switch writer\n",
    "    else:\n",
    "        writenew = False\n",
    "\n",
    "    # write chapter's content\n",
    "    if write:\n",
    "        chapters.append(line)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER I.\n",
      " Down the Rabbit-Hole\n",
      " \n",
      " \n",
      " Alice was beginning to get very tired of sitting by her sister on the\n",
      " bank, and of having nothing to do: once or twice she had peeped into\n",
      " the book her sister was reading, but it had no pictures or\n",
      " conversations in it, “and what is the use of a book,” thought\n"
     ]
    }
   ],
   "source": [
    "print(all_chapters[12:][0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we also captured `CHAPTER XX` in table of contents, we need to discard it and leave only the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude 12 first record\n",
    "all_chapters = all_chapters[12:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Remove:\n",
    "- Punctuation\n",
    "- Extra whitespaces\n",
    "- Non-Ascii\n",
    "- Number\n",
    "\n",
    "Preprocess:\n",
    "- Case folding\n",
    "- Tokenize\n",
    "- Stopword removal\n",
    "- Pos Tagging\n",
    "- Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaner\n",
    "def rm_punct(text):\n",
    "    return re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "\n",
    "def rm_whitespaces(text):\n",
    "    return re.sub(r' +', ' ', text)\n",
    "\n",
    "def rm_nonascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "def rm_number(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def clean_pipeline(text):\n",
    "    no_punct = rm_punct(text)\n",
    "    no_number = rm_number(no_punct)\n",
    "    no_whitespaces = rm_whitespaces(no_number)\n",
    "    no_nonascii = rm_nonascii(no_whitespaces)\n",
    "    return no_nonascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesser\n",
    "def casefold(text):\n",
    "    return text.lower()\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def rm_stopwords(text):\n",
    "    return [i for i in text if i not in stopwords]\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "    # make sure lemmas does not contains sotpwords\n",
    "    return rm_stopwords(lemmas)\n",
    "\n",
    "def preprocess_pipeline(text, join=True):\n",
    "    tokens = tokenize(casefold(text))\n",
    "    no_stopwords = rm_stopwords(tokens)\n",
    "    lemmas = lemmatize(no_stopwords)\n",
    "\n",
    "    return ' '.join(lemmas) if join else lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chapter rabbit hole alice beginning get tired sitting sister bank nothing twice peeped book sister reading picture conversation use book thought alice without picture conversation considering mind well could hot day made feel sleepy stupid whether pleasure making daisy chain would worth trouble gett'"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_txt = [clean_pipeline(chapter) for chapter in all_chapters]\n",
    "clean_txt = [preprocess_pipeline(chapter) for chapter in clean_txt]\n",
    "\n",
    "clean_txt[0][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we asked to find the top 10 important words from each chapter that is not `Alice` word.\n",
    "<br><br>\n",
    "So, first we need to remove `Alice` from all chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no alice\n",
    "no_alice = [' '.join([word for word in row.split(' ') if word.lower() != 'alice']) for row in clean_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abide</th>\n",
       "      <th>able</th>\n",
       "      <th>absence</th>\n",
       "      <th>absurd</th>\n",
       "      <th>acceptance</th>\n",
       "      <th>accident</th>\n",
       "      <th>accidentally</th>\n",
       "      <th>account</th>\n",
       "      <th>accounting</th>\n",
       "      <th>accusation</th>\n",
       "      <th>...</th>\n",
       "      <th>youall</th>\n",
       "      <th>youare</th>\n",
       "      <th>youd</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youve</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zigzag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032442</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031073</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031073</td>\n",
       "      <td>0.032146</td>\n",
       "      <td>0.021159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.026332</td>\n",
       "      <td>0.030661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020878</td>\n",
       "      <td>0.024921</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047850</td>\n",
       "      <td>0.015950</td>\n",
       "      <td>0.057114</td>\n",
       "      <td>0.140543</td>\n",
       "      <td>0.014463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.022229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.019090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018946</td>\n",
       "      <td>0.012470</td>\n",
       "      <td>0.012470</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027646</td>\n",
       "      <td>0.016499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017609</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051363</td>\n",
       "      <td>0.025682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 2373 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abide      able  absence    absurd  acceptance  accident  accidentally  \\\n",
       "0   0.000000  0.000000   0.0000  0.000000    0.000000  0.000000      0.000000   \n",
       "1   0.000000  0.031073   0.0000  0.000000    0.000000  0.000000      0.000000   \n",
       "2   0.000000  0.000000   0.0000  0.026332    0.030661  0.000000      0.000000   \n",
       "3   0.000000  0.000000   0.0000  0.000000    0.000000  0.000000      0.000000   \n",
       "4   0.000000  0.000000   0.0000  0.000000    0.000000  0.000000      0.000000   \n",
       "5   0.022229  0.000000   0.0000  0.019090    0.000000  0.000000      0.000000   \n",
       "6   0.000000  0.000000   0.0000  0.000000    0.000000  0.000000      0.000000   \n",
       "7   0.000000  0.000000   0.0000  0.000000    0.000000  0.000000      0.000000   \n",
       "8   0.000000  0.000000   0.0203  0.000000    0.000000  0.000000      0.000000   \n",
       "9   0.000000  0.000000   0.0000  0.000000    0.000000  0.000000      0.000000   \n",
       "10  0.000000  0.000000   0.0000  0.000000    0.000000  0.000000      0.000000   \n",
       "11  0.000000  0.000000   0.0000  0.000000    0.000000  0.051363      0.025682   \n",
       "\n",
       "     account  accounting  accusation  ...    youall    youare      youd  \\\n",
       "0   0.000000    0.000000     0.00000  ...  0.000000  0.000000  0.000000   \n",
       "1   0.000000    0.000000     0.00000  ...  0.000000  0.031073  0.032146   \n",
       "2   0.000000    0.000000     0.00000  ...  0.000000  0.000000  0.000000   \n",
       "3   0.000000    0.000000     0.00000  ...  0.025765  0.000000  0.013328   \n",
       "4   0.000000    0.000000     0.00000  ...  0.000000  0.000000  0.000000   \n",
       "5   0.000000    0.000000     0.00000  ...  0.000000  0.000000  0.000000   \n",
       "6   0.015728    0.000000     0.00000  ...  0.000000  0.000000  0.018946   \n",
       "7   0.018725    0.000000     0.00000  ...  0.000000  0.000000  0.022557   \n",
       "8   0.000000    0.000000     0.00000  ...  0.000000  0.000000  0.010501   \n",
       "9   0.000000    0.017609     0.00000  ...  0.000000  0.000000  0.009109   \n",
       "10  0.000000    0.000000     0.02263  ...  0.000000  0.000000  0.000000   \n",
       "11  0.000000    0.000000     0.00000  ...  0.000000  0.000000  0.013284   \n",
       "\n",
       "       youll     young     youre     youth     youve   zealand    zigzag  \n",
       "0   0.000000  0.000000  0.013184  0.000000  0.000000  0.032442  0.000000  \n",
       "1   0.021159  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.020878  0.024921  0.000000  0.000000  0.000000  0.000000  \n",
       "3   0.000000  0.000000  0.010471  0.000000  0.000000  0.000000  0.000000  \n",
       "4   0.047850  0.015950  0.057114  0.140543  0.014463  0.000000  0.023424  \n",
       "5   0.015136  0.000000  0.045167  0.000000  0.000000  0.000000  0.000000  \n",
       "6   0.012470  0.012470  0.007442  0.000000  0.000000  0.000000  0.000000  \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.013463  0.000000  0.000000  \n",
       "8   0.000000  0.027646  0.016499  0.000000  0.000000  0.000000  0.000000  \n",
       "9   0.000000  0.000000  0.007156  0.000000  0.021745  0.000000  0.000000  \n",
       "10  0.000000  0.000000  0.027590  0.000000  0.013973  0.000000  0.000000  \n",
       "11  0.000000  0.000000  0.010437  0.000000  0.015857  0.000000  0.000000  \n",
       "\n",
       "[12 rows x 2373 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "vec = tfidf.fit_transform(no_alice).toarray()\n",
    "\n",
    "df = pd.DataFrame(vec, columns=tfidf.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Most Important Words from each Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1: little, bat, door, rabbit, key, way, eat, hole, think, like\n",
      "Chapter 2: mouse, pool, little, im, swam, cat, dear, said, foot, mabel\n",
      "Chapter 3: said, mouse, dodo, race, prize, lory, dry, thimble, know, bird\n",
      "Chapter 4: bill, little, window, rabbit, puppy, chimney, glove, bottle, fan, said\n",
      "Chapter 5: caterpillar, said, pigeon, serpent, im, youth, egg, size, father, little\n",
      "Chapter 6: said, footman, cat, baby, mad, duchess, wow, like, pig, cook\n",
      "Chapter 7: hatter, dormouse, said, hare, march, tea, twinkle, time, draw, treacle\n",
      "Chapter 8: queen, said, hedgehog, king, gardener, soldier, cat, five, rose, executioner\n",
      "Chapter 9: turtle, said, mock, gryphon, duchess, moral, queen, went, school, say\n",
      "Chapter 10: turtle, mock, gryphon, said, lobster, dance, soup, beautiful, join, whiting\n",
      "Chapter 11: king, hatter, said, court, dormouse, witness, jury, queen, juror, officer\n",
      "Chapter 12: said, king, jury, queen, sister, slate, dream, would, rabbit, fit\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    top_10 = ', '.join(row.sort_values(ascending=False)[:N].index.tolist())\n",
    "    print(f'Chapter {i+1}: {top_10}')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Alice do most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['begin', 'get', 'sit', 'peep', 'read']"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_tag = ['VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
    "all_verbs = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for chapter in all_chapters:\n",
    "    # split sentences for each chapter\n",
    "    sent = sent_tokenize(casefold(chapter))\n",
    "\n",
    "    # iterate for tokenized sentences\n",
    "    for s in sent:\n",
    "        # only process if sentence contains 'alice'\n",
    "        if 'alice' in s:\n",
    "            # clean sentence\n",
    "            s_clean = clean_pipeline(s)\n",
    "            # preprocess sentence\n",
    "            s_prep = preprocess_pipeline(s_clean, join=False)            \n",
    "            # pos tagging sentence\n",
    "            s_tags = nltk.pos_tag(s_prep)\n",
    "            # filter verbs based on predefined tags\n",
    "            s_verbs = [verb for verb,tag in s_tags if tag in verbs_tag]\n",
    "            # lemmatize filtered verbs\n",
    "            s_lverbs = [lemmatizer.lemmatize(v, 'v') for v in s_verbs]\n",
    "            # save lemmatized verbs\n",
    "            all_verbs.append(s_lverbs)    \n",
    "\n",
    "# check results\n",
    "all_verbs[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Alice do most often:\n",
      "say, 295 times.\n",
      "go, 91 times.\n",
      "think, 59 times.\n",
      "get, 57 times.\n",
      "look, 48 times.\n",
      "come, 43 times.\n",
      "know, 42 times.\n",
      "begin, 41 times.\n",
      "see, 33 times.\n",
      "make, 32 times.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# flatten\n",
    "flatten_verbs = [x for l in all_verbs for x in l]\n",
    "\n",
    "# get top 10\n",
    "top10 = Counter(flatten_verbs).most_common(10)\n",
    "\n",
    "print('Top 10 Alice do most often:')\n",
    "for verb, freq in top10:\n",
    "    print(f'{verb}, {freq} times.')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26c34680807baa013af8072d8b13ca2200fd950cf2e6719fcefa2c8046b7cb6b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('detox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
